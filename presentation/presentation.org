#    -*- mode: org -*-
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:nil
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:nil reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:900
#+OPTIONS: toc:nil <:nil timestamp:nil email:t reveal_slide_number:"c/t"
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANS: slide
#+REVEAL_THEME: night
#+REVEAL_HLEVEL: 2
#+REVEAL_EXTRA_CSS: ./presentation.css
#+REVEAL_ROOT: ../../presentations/reveal.js/

# (setq org-reveal-title-slide "<h1>%t</h1><h2>Exploring an Algorithmic Empire</h2><br/><h2>%a</h2><h3>%e / <a href=\"http://twitter.com/ben_deane\">@ben_deane</a></h3><h2>%d</h2>")
# (setq org-reveal-title-slide 'auto)
# see https://github.com/yjwen/org-reveal/commit/84a445ce48e996182fde6909558824e154b76985

#+TITLE: std::accumulate
#+AUTHOR: Ben Deane
#+EMAIL: bdeane@blizzard.com
#+DATE: 29th July 2016

* Part 0
The Most Powerful Algorithm in the World?

** A Long Time Ago, In a Galaxy etc...
[[./powerful.png]]

** A Long Time Ago, In a Galaxy etc...
[[./another_talk.png]]

** Another Talk

This is that talk.

** Nomenclature note
I'll try to talk "in C++" for this talk.

But I may occasionally say "fold" when I mean "accumulate" or "reduce".

Don't worry... this talk isn't FP-heavy. There's no Haskell code.

* Part 1
Accumulatable Things

** ~std::accumulate~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <class InputIt, class T, class BinaryOp>
T accumulate(InputIt first, InputIt last,
             T init, BinaryOp op)
{
  for (; first != last; ++first) {
    init = op(init, *first);
  }
  return init;
}
#+END_SRC

#+BEGIN_NOTES
Our friend accumulate.

One thing to notice: it looks like there are a lot of copies going on here. In
fact compilers are pretty good at optimizing these away.

I took a look at this with an instrumented class that counts copies, moves, etc,
and compared it to a version that used explicit moves: there was no difference
except in the single copy of the passed-in init value.
#+END_NOTES

** Typical uses
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
vector<int> v = {1,2,3,4,5};

int sum = accumulate(v.cbegin(), v.cend(), 0, plus<>{});
cout << sum << '\n';

int product = accumulate(v.cbegin(), v.cend(), 1, multiplies<>{});
cout << product << '\n';
#+END_SRC
#+REVEAL_HTML: <br/>
#+ATTR_REVEAL: :frag appear
Of course, this is not why you're here.
What else can we accumulate?

#+BEGIN_NOTES
This is what many people think accumulate is for. And not much else?
#+END_NOTES

** How about finding a min or max?
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
vector<unsigned> v = {1,2,3,4,5};

unsigned max_val = accumulate(v.cbegin(), v.cend(), 0,
    [] (unsigned a, unsigned b) { return a > b ? a : b; });
cout << max_val << '\n';
#+END_SRC

#+BEGIN_NOTES
Are you convinced that this will find the maximum value?
#+END_NOTES

** How about finding a min or max?
Value-based ~min_element~ or ~max_element~
#+BEGIN_SRC cpp
template <typename It, typename Compare,
          typename T = typename std::iterator_traits<It>::value_type>
T min_element(It first, It last, Compare cmp)
{
  // precondition: first != last
  auto init = *first;
  return accumulate(
      ++first, last, init,
      [&] (const T& a, const T& b) {
        return cmp(b, a) ? b : a;
      });
}
#+END_SRC

#+BEGIN_NOTES
How about this finding the min value?

Note the use of the Compare function. Why did I write it this way?
#+END_NOTES

** What about ~bool~ values?
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
bool results[] = {true, false, true, true, false};

bool all_true = accumulate(cbegin(results), cend(results),
                           true, logical_and<>{});
bool some_true = accumulate(cbegin(results), cend(results),
                            false, logical_or<>{});
bool none_true = !accumulate(cbegin(results), cend(results),
                             false, logical_or<>{});
#+END_SRC
#+REVEAL_HTML: <br/>
#+ATTR_REVEAL: :frag appear
Not that interesting yet...

#+BEGIN_NOTES
Well.. this is just like all_of, any_of, none_of.

Except without the nice shortcut behaviour. Why would we do this? This isn't
very interesting as it stands.
#+END_NOTES

** The signature of the function
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
Type1 fun(const Type1 &a, const Type2 &b);
#+END_SRC
#+ATTR_REVEAL: :frag appear
So far, we've looked at ~Type1~ and ~Type2~ being the same.
#+ATTR_REVEAL: :frag appear
Things get more interesting when they differ.

** A more interesting ~bool~ case
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
map<int, weak_ptr<thing>> cache;

shared_ptr<thing> get_thing(int id) {
  auto sp = cache[id].lock();
  if (!sp) make_async_request(id);
  return sp;
}

void load_things(const vector<int>& ids)
{
  bool all_cached = accumulate(
    ids.cbegin(), ids.cend(), true,
    [] (bool cached, int id) {
      return get_thing(id) && cached;
    });
  if (!all_cached)
    service_async_requests();
}
#+END_SRC

#+BEGIN_NOTES
A simple cache inspired by Herb Sutter's favorite 10 lines of code.

We don't want to shortcut here. We want to enumerate all the things.

Note the order of args to &&.
#+END_NOTES

** ~bool~ As the Result
We use many function results as boolean values in control flow.
#+ATTR_REVEAL: :frag (appear)
 - actual ~bool~
 - pointers
 - zero-result of a comparison trichotomy
 - anywhere else we want to write ~if (x)~

#+ATTR_REVEAL: :frag appear
This means we can use ~accumulate~ to collect these function values. (Similar to
~all_of~, ~any_of~, ~none_of~, but where we don't want the short-circuiting
behavior.)

#+BEGIN_NOTES
We use a lot of things as bools.
#+END_NOTES

** More things...
#+ATTR_REVEAL: :frag (appear)
 - joining strings
 - building requests from key-value pairs
 - merging JSON objects
 - multiplying matrices

#+ATTR_REVEAL: :frag appear
What do all of these have in common?

#+BEGIN_NOTES
What indeed?

They can all be accumulated?
#+END_NOTES

** You All Remember Monoids?
A set of objects and an operation such that:
 - The operation is closed over the set
 - The operation is associative
 - There is an identity element

#+BEGIN_NOTES
"Monoid Fan"

"I <3 monoids"
#+END_NOTES

** Monoids Are Everywhere!
Monoids are everywhere, and any monoid can be accumulated:
#+ATTR_REVEAL: :frag (appear)
 - addition on integers
 - concatenation on strings
 - union on sets
 - "merging" objects of all kinds
 - max, min, and, or, ...
 - parsing
 - many more things...

#+BEGIN_NOTES
 - Parsing: mempty = fail, a `mappend` b = try a <|> b
 - Applying AI behaviours
 - Summing vectors
 - Composing bitmaps
 - Set intersections
 - optional

Simeon's JSON objects.
#+END_NOTES

** Building HTTP headers: Before
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
curl_slist* curl_headers = NULL;
for (auto it = headers.begin();
     it != headers.end(); ++it)
{
  curl_headers = curl_slist_append(curl_headers,
    (format("%s: %s") % it->first % it->second).str().c_str());
}
#+END_SRC

#+BEGIN_NOTES
A raw loop.

In fact it's OK for the headers to be null here.
#+END_NOTES

** Building HTTP headers: After
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
curl_slist* curl_headers = accumulate(
    headers.cbegin(), headers.cend(), static_cast<curl_slist*>(nullptr),
    [] (curl_slist* h, const auto& v) {
        return curl_slist_append(h,
          (format("%s: %s") % it->first % it->second).str().c_str());
      });
#+END_SRC

#+BEGIN_NOTES
The same thing, expressed as an accumulate.

Now there is no declaration/initialization split.

AAA style is more available.
#+END_NOTES

** Parallel Operations are Monoidal
A set of objects: parallel computations

An operation: "race" (parallelize)
#+ATTR_REVEAL: :frag appear
An identity value: the operation that never completes.

#+ATTR_REVEAL: :frag appear
Example use: UI cancellation

#+BEGIN_NOTES
What would the identity be?

(From my composing async behaviour talk)

This means that future<T> can be a monoid.
#+END_NOTES

** More Monoid Observations
#+ATTR_REVEAL: :frag appear
A type may be a monoid in more than one way (under more than one operation).
#+ATTR_REVEAL: :frag appear
A function that returns a monoid is a monoid.
#+ATTR_REVEAL: :frag appear
An aggregate of monoids is a monoid. (e.g. ~map<K,V>~ where ~V~ is a monoid)

#+BEGIN_NOTES
Addition/multiplication on ints.

Functions are monoids on their outputs.

Maps are monoids on their values.

(This is the same thing.)
#+END_NOTES

** Why not just write a loop?
Some advantages to ~accumulate~

#+ATTR_REVEAL: :frag (appear)
 - No declaration/initialization split
 - It's often easier to write a binary function
   - Or a unary function with monoidal output
   - Simplifies an API
 - Incremental computation
   - Can accumulate by parts
 - Potential for parallel computation

** What ~accumulate~ Can Do
#+ATTR_REVEAL: :frag (appear)
 - Turn binary functions into n-ary functions
 - Collect results of functions whose outputs are monoidal
 - Allow part-whole hierarchies to be treated uniformly
   - which unlocks parallel computation
 - And don't forget reverse iterators, too

* Part 2
Aside: Parallel Computations and Monoids

** Distributed Accumulate
[[./distributed_add.svg]]

#+BEGIN_NOTES
Identity + associativity.
#+END_NOTES

** Distributed Accumulate
[[./distributed_average.svg]]

#+BEGIN_NOTES
Sometimes we need to keep track of a pair of things, or more.

That's fine.
#+END_NOTES

** ~std::reduce~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <class InputIt, class T, class BinaryOp>
T reduce(InputIt first, InputIt last,
         T init, BinaryOp op);
#+END_SRC
#+ATTR_REVEAL: :frag appear
The same as ~accumulate~, except the collection may be processed in parallel.
#+ATTR_REVEAL: :frag appear
This works because of associativity (semigroup property).
#+ATTR_REVEAL: :frag appear
We lose the type variation, but gain parallelism.

#+BEGIN_NOTES
~reduce~ is new with C++17.

Every monoid is a semigroup.

But the type variation requires a linear computation.
#+END_NOTES

** Big Data Monoids Everywhere
#+ATTR_REVEAL: :frag (appear)
 - averages (regular or decayed)
 - top-N calculations
 - histograms
 - bloom filters
 - Gaussian distributions
 - count-min sketch
 - Hyperloglog

#+BEGIN_NOTES
Explain each. (ish)

HLL is next slide.
#+END_NOTES

** HyperLogLog
[[./hll.png]]

From http://content.research.neustar.biz/blog/hll.html

#+BEGIN_NOTES
Explain.
#+END_NOTES

** Algebraic Structures in Big Data
#+ATTR_REVEAL: :frag (appear)
 - monoids and semigroups are the key to parallelism
 - the ability to combine "summary data"
 - expensive training happens once

#+BEGIN_NOTES
Summary data: HLL registers, Gaussian curve, etc.

Details are beyond the scope of this talk.

But: combining summaries is much cheaper than combining data sets or re-running
training on data set combinations.
#+END_NOTES

* Part 3
Nonlinear Structures

** ~accumulate~ Works on Linear Sequences
How would we make it work on multi-dimensional structures?
#+ATTR_REVEAL: :frag appear
Maybe we can define a linear traversal on the structure (in-order, pre-order,
post-order)...
#+ATTR_REVEAL: :frag appear
But the nodes are still homogeneous...
#+ATTR_REVEAL: :frag appear
What if it's a bit more complex? (Like say, a JSON object?)

** Recall ~std::accumulate~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <class InputIt, class T, class BinaryOp>
T accumulate(InputIt first, InputIt last,
             T init, BinaryOp op);
#+END_SRC

The ~T~ here deals with an empty sequence.

The ~BinaryOp~ deals with a non-empty sequence.

#+BEGIN_NOTES
The insight here is that we really have two things we're dealing with.
#+END_NOTES

** Recursive Definition of a ~vector~
We can view "sequence accumulation" as handling two cases:
 - an empty ~vector~
 - a ~vector~ consisting of an element plus another ~vector~
#+ATTR_REVEAL: :frag appear
This is the sort of recursive definition we find in functional languages. And
it's the key to accumulating other data structures.

#+BEGIN_NOTES
The insight, explained.
#+END_NOTES

** ~std::accumulate~ Viewed Recursively
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename FwdIt, typename EmptyOp, typename NonEmptyOp>
auto recursive_accumulate(FwdIt first, FwdIt last,
                          EmptyOp op1, NonEmptyOp op2)
{
  if (first == last) return op1();
  return op2(*first, recursive_accumulate(first+1, last, op1, op2));
}
#+END_SRC
#+ATTR_REVEAL: :frag appear
~T~ (here ~EmptyOp~) is really a function from empty ~vector~ to ~T~
#+ATTR_REVEAL: :frag appear
~BinaryOp~ (here ~NonEmptyOp~) is really a function from (element, ~vector~) to
~T~

#+BEGIN_NOTES
If we look at the init value as a function, it becomes clearer how to deal with
sum types.
#+END_NOTES

** Accumulating a ~variant~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
struct JSONWrapper;
using JSONArray = vector<JSONWrapper>;
using JSONObject = map<string, JSONWrapper>;
using JSONValue = variant<bool,
                          double,
                          string,
                          nullptr_t,
                          JSONArray,
                          JSONObject>;
struct JSONWrapper
{
  JSONValue v;
  operator JSONValue&() { return v; }
  operator const JSONValue&() const { return v; }
};
#+END_SRC

#+BEGIN_NOTES
Simple implementation of a JSON value.

Wrapper provides for recursion. With implicit conversion to a value.
#+END_NOTES

** Example: Render a ~JSONValue~ as a ~string~
We need a function for each distinct type that can be inside the ~variant~.
#+BEGIN_SRC cpp
string render_json_value(const JSONValue& jsv);

string render_bool(bool b) { return b ? "true" : "false"; };
string render_double(double d) { return to_string(d); };
string render_string(const string& s)
{
  stringstream ss;
  ss << quoted(s);
  return ss.str();
}
string render_null(nullptr_t) { return "null"; }
#+END_SRC

** Example: Render a ~JSONValue~ as a ~string~
We need a function for each distinct type that can be inside the ~variant~.
#+BEGIN_SRC cpp
string render_array(const JSONArray& a)
{
  return string{"["}
    + join(a.cbegin(), a.cend(), string{","},
           [] (const JSONValue& jsv) {
             return render_json_value(jsv);
           })
    + "]";
}
#+END_SRC

** Example: Render a ~JSONValue~ as a ~string~
We need a function for each distinct type that can be inside the ~variant~.
#+BEGIN_SRC cpp
string render_object(const JSONObject& o)
{
  return string{"{"}
    + join(o.cbegin(), o.cend(), string{","},
           [] (const JSONObject::value_type& jsv) {
             return render_string(jsv.first) + ":"
               + render_json_value(jsv.second);
           })
    + "}";
}
#+END_SRC

** Example: Render a ~JSONValue~ as a ~string~
We need a function for each distinct type that can be inside the ~variant~.
#+BEGIN_SRC cpp
string render_json_value(const JSONValue& jsv)
{
  return fold(jsv,
              render_bool, render_double, render_string,
              render_null, render_array, render_object);
}
#+END_SRC

#+BEGIN_NOTES
Pass in a function for each distinct constructor.

Compare to accumulate with init as function.

OK, now we'll go through one way to write fold.
#+END_NOTES

** A Generic ~fold~ for ~variant~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename... Ts, typename... Fs>
auto fold(const variant<Ts...>& v, Fs&&... fs)
{
  static_assert(sizeof...(Ts) == sizeof...(Fs),
                "Not enough functions provided to variant fold");
  return fold_at<0, sizeof...(Ts)>::apply(
      v, v.index(),
      std::forward<Fs>(fs)...);
}
#+END_SRC
A ~variant~, and N functions (one for each case of the ~variant~).

Recall that the "zero value" is implicit in the functions if required.

#+BEGIN_NOTES
"Zero value" in accumulate is really in the "function" that is the init value.
#+END_NOTES

** A Generic ~fold~ for ~variant~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename T, typename F, typename... Fs>
static auto fold_at(T&& t, size_t n, F&& f, Fs&&... fs)
{
  using R = decltype(f(get<0>(t)));
  return apply_at<0, sizeof...(Fs)+1>::template apply<R, T, F, Fs...>(
      std::forward<T>(t),
      n,
      std::forward<F>(f),
      std::forward<Fs>(fs)...);
}
#+END_SRC

** A Generic ~fold~ for ~variant~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <size_t N, size_t Max>
struct apply_at
{
  template <typename R, typename T, typename F, typename... Fs>
  static auto apply(T&& t, size_t n, F&& f, Fs&&... fs)
  {
    if (n == N)
      return std::forward<F>(f)(get<N>(std::forward<T>(t)));
    else
      return apply_at<N+1, Max>::template apply<R, T, Fs...>(
          std::forward<T>(t),
          n,
          std::forward<Fs>(fs)...);
  }
};
#+END_SRC

#+BEGIN_NOTES
A recursive template. Louis could probably do better...
#+END_NOTES

** A Generic ~fold~ for ~variant~
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <size_t Max>
struct apply_at<Max, Max>
{
  template <typename R, typename T, typename... Fs>
  static auto apply(T, size_t, Fs...)
  {
    assert("Variant index out of range" && false);
    return R{};
  }
};
#+END_SRC

#+BEGIN_NOTES
This function is actually never called (because of the static_assert earlier).

But it's required to compile and terminate the type recursion.
#+END_NOTES

** Generic ~variant~ accumulation
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename... Ts, typename... Fs>
auto fold(const variant<Ts...>& v, Fs&&... fs)
#+END_SRC
Hmm, this looks a lot like visitation.

#+BEGIN_NOTES
Visitation on a single variant, except visit is defined with a single visitor
class that implements a function for each type in the variant.
#+END_NOTES

** Recursive reduction
Any recursively-specified data structure can be accumulated using visitation to
produce a monoidal value which is accumulated
#+ATTR_REVEAL: :frag (appear)
 - tree -> string rendering
 - depth, fringe of trees
 - lighting contributions
 - scene graph operations

#+ATTR_REVEAL: :frag appear
This is a useful alternative when dealing with heterogeneous hierarchies that it
is difficult to define a linear traversal for.

* Part 4
Heterogeneous Sequences

** Beyond Monoids
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <class InputIt, class T, class BinaryOp>
T accumulate(InputIt first, InputIt last,
             T init, BinaryOp op);

Type1 fun(const Type1 &a, const Type2 &b);
#+END_SRC
~Type1~ and ~Type2~ can be different: this is saying that we know how to "fold"
values of ~Type2~ into values of ~Type1~.

** Heterogeneous Folding
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename T>
Accumulator fun(const Accumulator &a, const T &b);
#+END_SRC
What if ~T~ varied all the time? We could have cases where we know how to "fold"
lots of different types into an accumulated value.

#+BEGIN_NOTES
There is one obvious example of this that is almost ubiquitous.
#+END_NOTES

** The Obvious Example
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename T>
ostream& operator<<(ostream& s, const T &t);
#+END_SRC

** The Obvious Example
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
auto t = make_tuple("Hello", 3.14, 1729, 'a');
auto f = [] (ostream& s, const auto& x) -> ostream& {
  return s << x << '\n';
};
fold(t, cout, f) << "done" << endl;
#+END_SRC
#+BEGIN_SRC bash
$ ./a.out
Hello
3.14
1729
a
done
$
#+END_SRC

#+BEGIN_NOTES
We supply the init value here (cout).

Compare implicit init value in functions for variant. What is the difference
here?

We could have structured the function to take a monostate sentinel for the
"empty tuple" before the first element. But this is cleaner.
#+END_NOTES

** Heterogeneous Folding
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename F, typename Z, typename... Ts>
decltype(auto) fold(const tuple<Ts...>& t, Z&& z, F&& f);
#+END_SRC
(Implementation left as an exercise)

#+BEGIN_NOTES
Not too difficult, but it takes some trickiness to deal with nonmovable types like ostream.
#+END_NOTES

** Different Types of Accumulation
#+ATTR_REVEAL: :frag (appear)
 - std::accumulate
   - 1 function, linear homogeneous structure
 - std::accumulate with linear tree traversal
   - 1 function, multidimensional homogeneous structure
 - tuple-fold
   - n functions, linear heterogeneous structure
 - variant-fold
   - n functions, multidimensional heterogeneous structure

#+ATTR_REVEAL: :frag appear
The empire so far... all of these could also be parallel, given the appropriate
monoidal structure.

* Part 5
The Opposite of Accumulate?

** Fold? Unfold
If ~accumulate~ is folding up a data structure to produce a value...
#+ATTR_REVEAL: :frag appear
The opposite is "unfolding" a seed value to produce a data structure.

** How To Unfold
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename InputIt, typename T, typename F>
T accumulate(InputIt first, InputIt last, T init, F f);

template <typename OutputIt, typename T, typename F>
OutputIt unfold(F f, OutputIt it, T init);
#+END_SRC
~F~ will be repeatedly called with a "reducing" ~T~ value and write the result(s)
to ~it~.

#+ATTR_REVEAL: :frag (appear)
 - What should the signature of F be?
 - How do we know when we're done?

** Signature of the Function Passed to ~unfold~
F is the opposite of ~accumulate~'s BinaryOp
#+BEGIN_SRC cpp
Type1 fun(const Type1 &a, const Type2 &b);
#+END_SRC
#+ATTR_REVEAL: :frag appear
It's clear that ~F~ needs to return a ~pair~
#+ATTR_REVEAL: :frag (appear)
 * result (say of type ~U~) to write into the output range
 * new value of ~T~ to feed into next invocation of ~F~

#+ATTR_REVEAL: :frag appear
In general the "result to write to the iterator" may be a range or sequence of
values.

** Three Choices for ~unfold~ Termination
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
// 1. provide a sentinel value of type T
template <typename FwdIt, typename T, typename F>
FwdIt unfold(F f, FwdIt it, T init, T term);
#+END_SRC
Choice 1: terminate when a termination value (of type ~T~) is returned.

** Three Choices for ~unfold~ Termination
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
// 2. provide a sentinel value of type (other thing returned by F)
template <typename FwdIt, typename T, typename F, typename U>
FwdIt unfold(F f, FwdIt it, T init, U term);
#+END_SRC
Choice 2: terminate when a termination value (of type ~U~) is returned.

#+BEGIN_NOTES
Choices 1 and 2 both involve putting a sentinel termination value inside the
type: either in the first element or the second element of the returned pair.
#+END_NOTES

** Three Choices for ~unfold~ Termination
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
// 3. F will return an optional
template <typename FwdIt, typename T, typename F>
FwdIt unfold(F f, FwdIt it, T init);
#+END_SRC
Choice 3: F returns an ~optional~; terminate when ~nullopt~ is returned.

#+BEGIN_NOTES
optional is made for sentinel values!
#+END_NOTES

** How To Unfold
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <typename FwdIt, typename T, typename F>
FwdIt unfold(F f, FwdIt it, T init)
{
  T t{std::forward<T>(init)};
  for (auto o = f(t); o; o = f(o->second)) {
    it = std::move(std::begin(o->first), std::end(o->first), it);
  }
  return it;
}
#+END_SRC
~F~ returns ~optional<pair<range, T>>~

#+BEGIN_NOTES
Note that f returns a range. This is more general than returning a singleton
value (although it may be a singleton range)
#+END_NOTES

** Unfold Example
#+BEGIN_SRC cpp -n
optional<pair<string, int>> to_roman(int n)
{
  if (n >= 1000) return {{ "M", n-1000 }};
  if (n >= 900) return {{ "CM", n-900 }};
  if (n >= 500) return {{ "D", n-500 }};
  if (n >= 400) return {{ "CD", n-400 }};
  if (n >= 100) return {{ "C", n-100 }};
  if (n >= 90) return {{ "XC", n-90 }};
  if (n >= 50) return {{ "L", n-50 }};
  if (n >= 40) return {{ "XL", n-40 }};
  if (n >= 10) return {{ "X", n-10 }};
  if (n >= 9) return {{ "IX", n-9 }};
  if (n >= 5) return {{ "V", n-5 }};
  if (n >= 4) return {{ "IV", n-4 }};
  if (n >= 1) return {{ "I", n-1 }};
  return nullopt;
}
#+END_SRC

#+BEGIN_NOTES
A simple example. This function is quite easy to write?
#+END_NOTES

** Unfold Example
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
int main()
{
  string r;
  unfold(to_roman, back_inserter(r), 1729);
  cout << r << '\n';
}
#+END_SRC
#+BEGIN_SRC bash
$ ./a.out
MDCCXXIX
$
#+END_SRC

#+BEGIN_NOTES
Iterating to_roman gives us the expected result.
#+END_NOTES

** Fold and Unfold are Really the Same
Just transformations on a data structure.
#+ATTR_REVEAL: :frag appear
Which you use is a matter of convenience.
#+ATTR_REVEAL: :frag appear
We think of ~accumulate~ as working on structures and producing values, and
~unfold~ vice versa.
#+ATTR_REVEAL: :frag appear
But structures are themselves values...

#+BEGIN_NOTES
Compare ~generate~, ~generate_n~
#+END_NOTES

* Postscript
The Fruits of Algorithmic Perversions

** The Question
If you were stuck on a desert island, which algorithms would you take with you?
#+ATTR_REVEAL: :frag appear
Maybe some "building block" algorithms?
#+ATTR_REVEAL: :frag (appear)
 - ~partition~
 - ~rotate~
 - ~reverse~
#+ATTR_REVEAL: :frag appear
Maybe some others?
#+ATTR_REVEAL: :frag appear
Which algorithms are the most powerful?
#+ATTR_REVEAL: :frag appear
What if you couldn't write any loops, so you're stuck with what you have?

** The Algorithms (pre-C++17)
#+REVEAL_HTML: <div id="columns"> <div id="fiveup">
#+ATTR_REVEAL: :frag (none none none none none highlight-red none none none none none none none highlight-red none none none none none none none none none none none none none none none none none none none none fade-out none highlight-red highlight-red none none none none none none none none none none none none none highlight-red highlight-red none none none highlight-red highlight-red none highlight-red fade-out none none none none none none none none none none none none none none none none none none none none none none none fade-out none none none none highlight-red) :frag_idx (1 1 1 1 1 4 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 4 5 1 1 1 1 1 1 1 1 1 1 1 1 1 5 5 1 1 1 4 5 1 5 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 4)
 - ~accumulate~
 - ~adjacent_difference~
 - ~adjacent_find~
 - ~all_of~
 - ~any_of~
 - ~binary_search~
 - ~copy~
 - ~copy_backward~
 - ~copy_if~
 - ~copy_n~
 - ~count~
 - ~count_if~
 - ~equal~
 - ~equal_range~
 - ~fill~
 - ~fill_n~
 - ~find~
 - ~find_end~
 - ~find_first_of~
 - ~find_if~
 - ~find_if_not~
 - ~for_each~
 - ~generate~
 - ~generate_n~
 - ~includes~
 - ~inner_product~
 - ~inplace_merge~
 - ~iota~
 - ~is_heap~
 - ~is_heap_until~
 - ~is_partitioned~
 - ~is_permutation~
 - ~is_sorted~
 - ~is_sorted_until~
 - ~iter_swap~
 - ~lexicographical_compare~
 - ~lower_bound~
 - ~make_heap~
 - ~max~
 - ~max_element~
 - ~merge~
 - ~min~
 - ~min_element~
 - ~minmax~
 - ~minmax_element~
 - ~mismatch~
 - ~move~
 - ~move_backward~
 - ~next_permutation~
 - ~none_of~
 - ~nth_element~
 - ~partial_sort~
 - ~partial_sort_copy~
 - ~partial_sum~
 - ~partition~
 - ~partition_copy~
 - ~partition_point~
 - ~pop_heap~
 - ~prev_permutation~
 - ~push_heap~
 - ~random_shuffle~
 - ~remove~
 - ~remove_copy~
 - ~remove_copy_if~
 - ~remove_if~
 - ~replace~
 - ~replace_copy~
 - ~replace_copy_if~
 - ~replace_if~
 - ~reverse~
 - ~reverse_copy~
 - ~rotate~
 - ~rotate_copy~
 - ~search~
 - ~search_n~
 - ~set_difference~
 - ~set_intersection~
 - ~set_symmetric_difference~
 - ~set_union~
 - ~shuffle~
 - ~sort~
 - ~sort_heap~
 - ~stable_partition~
 - ~stable_sort~
 - ~swap~
 - ~swap_ranges~
 - ~transform~
 - ~unique~
 - ~unique_copy~
 - ~upper_bound~
#+REVEAL_HTML: </div></div>

#+ATTR_REVEAL: :frag appear
Of 90 total, 77 are basically "plain loops"

#+BEGIN_NOTES
Remove swap, iter_swap

Remove random_shuffle (don't want STL to make angry face)

Remove binary searches

Remove heap ops

The rest!
#+END_NOTES

** Why Doesn't ~accumulate~ Work on Iterators?
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <class InputIt, class T, class BinaryOp>
T accumulate(InputIt first, InputIt last,
             T init, BinaryOp op)
{
  for (; first != last; ++first) {
    init = op(init, *first);
  }
  return init;
}
#+END_SRC

#+BEGIN_NOTES
It does work on iterators, doesn't it?
#+END_NOTES

** Why Doesn't ~accumulate~ Work on Iterators?
#+REVEAL_HTML: <br/>
#+BEGIN_SRC cpp
template <class InputIt, class T, class BinaryOp>
T accumulate(InputIt first, InputIt last,
             T init, BinaryOp op)
{
  for (; first != last; ++first) {
    init = op(init, first);
  }
  return init;
}
#+END_SRC
With this formulation, we can view an iterator as an accumulator value.

#+BEGIN_NOTES
Pass iterator to op rather than value to op: allows accumulating an iterator.
#+END_NOTES

** Accumulate abuse
I'm the first to admit that some of these algorithm implementations are...
interesting.

 - find (using exceptions for control flow)
 - reverse (using forward iterators and a function as the accumulator)

** But...
Some interesting alternatives arise.
#+ATTR_REVEAL: :frag (appear)
 - ~find_if~ -> ~find_all~?
 - ~adjacent_find~ -> ~adjacent_find_all~?
 - ~min_element~ that returns an ~optional~ value?
 - ~sort~ with forward iterators?

#+BEGIN_NOTES
Parallelism?

There has also been some interest in relaxing the iterator categories on some
algorithms. P0227 Weakening the iterator categories of some standard algorithms.
#+END_NOTES

** Hunting for (Raw?) Loops, Redux
Almost everything can be expressed as some form of accumulation.

Should it be? That's for you to decide.

But when you get used to seeing monoids, everything is monoids.

#+BEGIN_NOTES
Benefits: no declaration/initialization split

Watch out for: copies
#+END_NOTES

** Some Further Reading
 - [[http://www.amazon.com/exec/obidos/ASIN/032163537X/ref=nosim/elbenocom-20][Elements of Progamming]]
 - [[https://wiki.haskell.org/Fold][Fold (Haskell Wiki)]]
 - [[http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/][An Introduction to Recursion Schemes]]
 - [[https://izbicki.me/blog/gausian-distributions-are-monoids][Gaussian Distributions form a Monoid]]
 - [[https://github.com/twitter/algebird][Algebird]] / [[https://github.com/mikeizbicki/HLearn][HLearn]]
 - [[https://www.infoq.com/presentations/abstract-algebra-analytics][Add ALL the Things: Abstract Algebra Meets Analytics]]

 - Variant folding code
   - https://gist.github.com/elbeno/e5c333fff247e78990e08ab8ed56aecd
 - Tuple folding code
   - https://gist.github.com/elbeno/95e710c04d56d8cc72ade2e6b2bbe9d5

** Summary
#+ATTR_REVEAL: :frag (appear)
 - Any time you write an API, see if any of your data types form a monoid or a
   semigroup under any operations you provide.
 - Look for opportunities where you are applying a function in a loop.
 - Monoids are everywhere.
 - Think about folding with multidimensional structures or heterogeneous sequences.
 - Unfolds are an alternative way to think of things; can be combined with folds
   to produce arbitrary structural transformations.
 - Algorithmic perversions can be fruitful in terms of learning...
 - Accumulation: not just for adding things up!

